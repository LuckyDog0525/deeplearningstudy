{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNV2Ra9RK2aV",
        "outputId": "64a34b78-32db-4d0a-b367-ab121dcde3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#구글드라이브를 마운트한다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TddBZIQm7gXE",
        "outputId": "96ca258b-2898-44b8-f8c1-59c3f6631343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as tc\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "w1oKwxEC8B-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파이썬의 list 와 float 타입의 텐서를 통한 초기화\n",
        "arr = [1,2]\n",
        "mytensor = tc.tensor(arr)\n",
        "val = 2.0\n",
        "mytensor2 = tc.tensor(val)\n",
        "print(mytensor)\n",
        "print(mytensor2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpbaScoz8Uid",
        "outputId": "f0b81439-1840-48b9-f528-c253c430379d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2])\n",
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nparr = np.array([1,2])\n",
        "x_t = tc.from_numpy(nparr)\n",
        "print(nparr)\n",
        "print(x_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM3Mf4C08yu9",
        "outputId": "5566c8f0-4027-4709-e259-7b46f3a1cb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2]\n",
            "tensor([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zerotensor = tc.zeros((2,3)) #2 by 3 텐서를 0으로 초기화\n",
        "onetensor = tc.ones((2,3)) #2 by 3 텐서를 1로 초기화\n",
        "randtensor = tc.randn((2,3)) #2 by 3 텐서를 난수로 초기화,randn 에서 초기화된 각 element인 x는 -1<= x <=1 임\n",
        "print(zerotensor)\n",
        "print(onetensor)\n",
        "print(randtensor)\n",
        "zerotensor.shape #tensor 의 사이즈를 반환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3loKUbZ9IpY",
        "outputId": "9e2bbe60-1c70-4ede-8682-07cea5f3470a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[-0.4315,  1.7914, -0.1661],\n",
            "        [-0.2288,  0.1864,  0.5186]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_val = tc.tensor(val)\n",
        "t_val.dtype #tensor 의 데이터 타입을 반환 (float32) 여기서 float 32 는 4바이트(32비트)로 표현되는 부동소수점 숫자를 의미함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsiitygf9yyt",
        "outputId": "6324caa2-0b8d-4820-b8b4-95e18eba1b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2 = [1,2]\n",
        "mytensor3 = tc.tensor(arr2)\n",
        "print(mytensor3.dtype)\n",
        "mytensor4 = tc.tensor(arr2,dtype=tc.float32) #텐서 초기화시 데이터타입을 설정할 수 있다\n",
        "print(mytensor4.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCnFUnnC-gcn",
        "outputId": "610f4c35-8867-4208-dff4-4d1a0084afda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mytensor3.device)\n",
        "'''텐서가 할당된 장치를 알 수 있다 (cpu) 기본적으로 텐서는 cpu 에 할당된다\n",
        "런타임 유형 변경을 통해 GPU가 활성화된 상태라면 CUDA가 설치되어 있으므로 torch.cuda.is_available()로 CUDA 사용 가능 여부를 확인하고 사용할 수 있다.\n",
        "GPU가 사용 가능하다면, torch.device('cuda')로 장치 객체를 생성한 뒤, 텐서를 GPU에 할당할 수 있다.'''\n",
        "if tc.cuda.is_available():\n",
        "  mydevice = tc.device('cuda') #cuda 장치 객체 생성\n",
        "  mytensor5 = tc.tensor(arr2,device=mydevice) #tensor 생성시 device를 cuda 로 설정\n",
        "  print(mytensor5.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVJbg-rxAwaO",
        "outputId": "36849708-cfc0-402b-cb3f-e21c6eb2ae2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#텐서의 기본연산\n",
        "c = 10\n",
        "print(c*mytensor3) #스칼라곱\n",
        "\n",
        "print(onetensor + randtensor)#텐서의 덧셈\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNS-vjO3Cktz",
        "outputId": "807a2d9d-c158-48ec-f1cc-1ac2e5e662e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 20])\n",
            "tensor([[0.5685, 2.7914, 0.8339],\n",
            "        [0.7712, 1.1864, 1.5186]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brdtensor = tc.tensor([1,2,3])\n",
        "brdtensor2 = tc.tensor([[1,2,3],[3,4,5]])\n",
        "print(brdtensor*brdtensor2)\n",
        "''' 브로드 캐스팅으로 brdtensor 의 shape 은 (3,) 이지만 brdtensor2 와의 연산을 위해 shape이 (2,3) 이므로 전환된다\n",
        "이때 1행의 원소 [1,2,3]을 그대로 가지고 오므로 brdtensor 는 [[1,2,3],[1,2,3]] 이 된다\n",
        "추가로 텐서의 연산규칙은 행렬곱의 연산방식(Matrix Multiplication)인 앞 행렬의 행벡터와 뒷행렬의 열벡터를 내적하는것이 아닌 원소별 곱셈(element-wise Multiplication)이다\n",
        "즉 결과텐서의 1행인 [3,8,15]는 [1*3, 2*4, 3*5] 이다.'''\n",
        "#그 외 알면 용이한 메서드 몇 개\n",
        "brdtensor3 = tc.arange(9)\n",
        "#arange 함수는 파이썬의 range 와 같음\n",
        "print(brdtensor3.reshape(3,3))\n",
        "#reshape 함수는 3by3dmfh 텐서를 재배열 해준다\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bqLFB2NEDfh",
        "outputId": "33927fc1-d42a-4cb0-ec04-7279e3b16aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1,  4,  9],\n",
            "        [ 3,  8, 15]])\n",
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#행렬곱을 해주는 함수 matmul\n",
        "# tc.matmul(텐서1,텐서2) 혹은 텐서1@텐서2 와 같은식으로 적는다\n",
        "print(tc.matmul(newtensor,newtensor2))\n",
        "print(newtensor@newtensor2)\n",
        "#둘의 연산 결과가 같음을 알 수 있다."
      ],
      "metadata": {
        "id": "-jO0rmPDHLAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2cf030-c8c6-4f8b-bfad-85f829dc7b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.9020, -1.2905,  1.4520],\n",
            "        [-0.5196,  0.4843, -2.7218],\n",
            "        [-0.0078,  0.5390,  3.8282]])\n",
            "tensor([[ 0.9020, -1.2905,  1.4520],\n",
            "        [-0.5196,  0.4843, -2.7218],\n",
            "        [-0.0078,  0.5390,  3.8282]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#색인을 통한 접근\n",
        "# 색인화 : 텐서의 특정 element 에 접근을 용이하게 한다 보통 텐서는 행렬의 배열이므로 i,j,k 혹은 그 이상의 색인이 필요하다 (3차원 이상)\n",
        "newtensor3 = tc.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]])\n",
        "#여기서 텐서는 [행렬0,행렬1] 의 형태이고 행렬0 = [[3,7,9],[2,4,5]] 의 2by3 행렬이고  행렬1 = [[8,6,2],[3,9,1]] 이다\n",
        "#여기서 색인 i,j,k = 0,1,1 이라고 하면 i는 행렬 0을 가르키고 j와 k 는 행렬 0의 (1,1) element 를 가르킨다 즉 4를 가르킨다\n",
        "i,j,k = 0,1,1\n",
        "print(newtensor3[i,j,k])\n",
        "# : 는 해당 차원의 데이터 부분 집합 전체를 가져오는 역할임 즉 행렬0의 모든 행 모든 열을 반환함 즉 0~끝까지를 의미함\n",
        "# 시작 : 끝 의 형태로 쓴다\n",
        "print(newtensor3[0,:,:])\n",
        "print(newtensor3[0])\n",
        "# 색인으로 0,j:,: 를 쓰면 1행 이상의 모든 원소를 반환한다\n",
        "print(newtensor3[0,j:,:])\n",
        "# 또 0, :j , : 를 쓰면 1행 까지의 즉 1미만의 모든 원소를 반환한다\n",
        "print(newtensor3[0,:j,:])\n",
        "#특정 원소를 수정하는것도 가능함 (5를 1로 수정)\n",
        "newtensor3[0,1,2] = 1\n",
        "print(newtensor3)\n",
        "#또 행렬의 부분행렬을 새로운 행렬로 대체 가능함\n",
        "subtensor = tc.randn((2,3))\n",
        "newtensor3[0,:,:] = subtensor\n",
        "print(newtensor3)\n",
        "#여기서 반환값이 -1에서 1사이 난수가 아닌 -1,0,1,로 나오는것은 dtype 이 float 이 아닌 정수형이기 때문임 따라서 dtype 을 float 으로 바꿔주면\n",
        "newtensor4 = tc.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]],dtype = float)\n",
        "newtensor4[0,:,:] = subtensor\n",
        "print(newtensor4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXoVv1Pv9OSI",
        "outputId": "2089fd8f-4293-4c93-e8b2-abd5b34aad08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4)\n",
            "tensor([[3, 7, 9],\n",
            "        [2, 4, 5]])\n",
            "tensor([[3, 7, 9],\n",
            "        [2, 4, 5]])\n",
            "tensor([[2, 4, 5]])\n",
            "tensor([[3, 7, 9]])\n",
            "tensor([[[3, 7, 9],\n",
            "         [2, 4, 1]],\n",
            "\n",
            "        [[8, 6, 2],\n",
            "         [3, 9, 1]]])\n",
            "tensor([[[ 1,  1,  0],\n",
            "         [ 0,  0, -1]],\n",
            "\n",
            "        [[ 8,  6,  2],\n",
            "         [ 3,  9,  1]]])\n",
            "tensor([[[ 1.4866,  1.1927,  0.8449],\n",
            "         [ 0.1810, -0.5032, -1.7508]],\n",
            "\n",
            "        [[ 8.0000,  6.0000,  2.0000],\n",
            "         [ 3.0000,  9.0000,  1.0000]]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자동미분\n",
        "파이토치의 자동미분 라이브러리를 이용하면 함수의 도함수(혹은 편도함수)를 직접 구하지 않고 구할 수 있다. 그 대신 주어진 \"지점\"(입력값인 벡터)에서의 미분계수(혹은 편미분계수) 값을 구한다. 즉, 자동미분 라이브러리는 지정해준 점에서의 함수의 \"순간 기울기\"를 구하는 기능만을 갖고 있다. 이를 이용하면 미분가능한 모든 함수의 각 지점에서의 기울기를 구할 수 있다.\n",
        "\n",
        "# 텐서 객체의 '.backward()' 메서드\n",
        "파이토치에서 자동미분을 호출하는 메서드는 Tensor.backward()이다. \"다른 텐서로부터 계산한 텐서 F\"에 대해 F.backward()를 호출하면 파이토치는 F가 의존하는 모든 텐서에 대해 F의 편미분계수를 계산하도록 지시한다. 이 편미분계수들은 각각의 텐서들의 .grad 속성에 Tensor로 저장된다. 이때\n",
        "\n",
        "requires_grad=True 를 통해 매개변수 requires_grad 값을 True 로 함으로써 자동 미분 추적을 허용하도록 설정해야한다.\n",
        "\n",
        "backward() 호출: Tensor.backward()를 호출하면 해당 텐서로부터 계산된 모든 텐서에 대해 그래디언트(기울기)를 계산합니다. backward()는 스칼라 값에 대해서만 호출할 수 있습니다. 만약 텐서가 스칼라가 아니라면, 적절한 축소 연산을 통해 스칼라로 변환한 후 backward()를 호출해야 한다 (예: sum()).\n",
        "\n",
        "예를 들어 아래와 같이 x, y, z 텐서가 있고, x,y,z의 함수로 정의된 f 텐서가 있는 상황에서의 편미분을 살펴보자."
      ],
      "metadata": {
        "id": "08cOLxsXTW1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#파이토치를 활용한 자동미\n",
        "x = tc.tensor(2.0, requires_grad = True) #상수텐서 x,y,z 생성\n",
        "y = tc.tensor(3.0, requires_grad = True)\n",
        "z = tc.tensor(1.5, requires_grad = True)\n",
        "\n",
        "f = x**2 + y**2 + z**2\n",
        "f.backward()\n",
        "x.grad, y.grad, z.grad #각각 x,y,z 에 대한 편미분 계수이다\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YadywDg6Dcx8",
        "outputId": "1d6d4584-bf66-4f71-c2a2-ec8814b05c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4.), tensor(6.), tensor(3.))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "f.backward()를 호출하게 되면 파이토치가 f의 각 변수에 대한 모든 편미분계수를 계산하도록 지시한다. 이는 역전파(backpropagation)과정에서의 연쇄법칙(chain rule)에 기반한 알고리즘을 사용하여 수행된다.\n",
        "\n",
        "편미분 계수 반환\n",
        "x, y, z의 .grad() 속성을 살펴보면  $\\frac{\\partial f}{\\partial x}$,  $\\frac{\\partial f}{\\partial y}$, $\\frac{\\partial f}{\\partial z} $ 의 값을 얻을 수 있다. .grad()는 텐서로 구해진다."
      ],
      "metadata": {
        "id": "5pB5nILlVoyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파이토치 nn 모듈\n",
        "파이토치의 nn 모듈은 신경망(Neural Networks)을 구성하는 계층(layer), 손실 함수(loss function), 활성화 함수(activation function) 등을 사용할 수 있도록 패키징한 모듈이다. 즉, nn 모듈은 신경망을 학습시키기 위한 다양한 구성 요소들을 제공하는 API이다."
      ],
      "metadata": {
        "id": "yBzAE1x_YSao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "LwCytVc2XiS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dimention,output_dimention = 256,10\n",
        "vector = tc.randn(256)\n",
        "layer = nn.Linear(input_dimention, output_dimention, bias = True) #입력과 출력의 차원 결정 256, 10\n",
        "output_vector = layer(vector)\n",
        "print(output_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDPoDGRRZvMv",
        "outputId": "75b9cc60-1cdd-47ff-e1a6-4e07b78d88ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.1567, -1.0292,  0.5297,  0.6268, -0.2615, -0.7089,  0.2831, -0.0136,\n",
            "         0.1801, -1.3455], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vector = tc.randn(256)을 통해서 -1 에서 1 사이의 평균이 0 이고 표준편차가 1이 되는 256개의 난수들을 원소로 하는 열벡터 생성하고 이를 입력값으로 사용한다\n",
        "nn.Linear 을 통해서 입력받는 벡터의 차원과 출력하는 벡터의 차원을 결정한다 여기서 nn.Linear 은 선형변환이다.\n",
        "내부적으로는 (10, 256) 크기의 가중치 행렬 W가 만들어진다.\n",
        "즉, outputvector= W@vector+bias 이다.\n",
        "여기서 W는 (output_dimension=10, input_dimension=256) 크기의 가중치 행렬이다.\n",
        "또, b는 (output_dimension,) 크기의 편향 벡터. 즉, 길이가 10이고 1차원인, 편향 벡터이다.\n",
        "\n",
        "아래코드는 위 과정이 내부적으로 이루어지는 것을 수동으로 1개 층에 대해서 표현한것이다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_rjoZUGLeABE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = tc.rand(10,256)\n",
        "b = tc.zeros(10,)\n",
        "output = W@vector+b\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C314IuY3f2FP",
        "outputId": "32600a7a-f934-44f1-9828-94dc4d438807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-16.5068,  -3.7954, -10.3025,  -9.3129, -11.6612,  -5.5093, -15.3803,\n",
            "          4.4335,  -8.9609,  -8.8605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 1차적으로 나온값들을 활성화 함수(은닉층 혹은 중간레이어에서 사용되는 함수로 ReLU 함수, Leaky ReLU함수, Sigmoid 함수 ,하이퍼볼릭 탄젠트 함수 등등)에 대입한 값을 layer2 의 입력값으로 받는다. 이와 같은 작업을 반복적으로 수행한 후 최종적으로는 SoftMax 함수(출력층에서 쓰이는 함수)를 통해 반환된 값을 사용한다.\n",
        "\n",
        "2개층을 통해서 구현해보자. 또 ReLU 함수가 있을때와 없을때 반환값을 비교해보자."
      ],
      "metadata": {
        "id": "Wj1Bta2tim38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indim,featuredim,outdim = 784,256,10\n",
        "vec = tc.randn(784)\n",
        "layer1 = nn.Linear(indim,featuredim,bias=True)\n",
        "layer2 = nn.Linear(featuredim,outdim,bias=True)\n",
        "output = layer2(layer1(vec))\n",
        "print(output)\n",
        "relu = nn.ReLU()\n",
        "newoutput = layer2(relu(layer1(vec)))\n",
        "print(newoutput)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRbnIwPVjuL4",
        "outputId": "bf950c9d-0c72-46ff-a150-13d69d70f8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0066,  0.9858,  0.4957,  0.5508, -0.0784, -0.7237, -0.3587, -0.3233,\n",
            "        -0.2115, -0.5266], grad_fn=<ViewBackward0>)\n",
            "tensor([-0.1012,  0.5412,  0.1775,  0.4222,  0.0132, -0.3693, -0.3719, -0.3362,\n",
            "        -0.3097, -0.2944], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Module 클래스\n"
      ],
      "metadata": {
        "id": "E5FeFSLokzLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseClassifier(nn.Module):\n",
        "  def __init__(self,indim,featuredim,outdim):\n",
        "    super(BaseClassifier,self).__init__()\n",
        "    self.layer1 = nn.Linear(indim,featuredim,bias=True)\n",
        "    self.layer2 = nn.Linear(featuredim,outdim,bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    out = self.layer2(x)\n",
        "    return out\n",
        "\n",
        "no_examples = 10\n",
        "indim = 784\n",
        "featuredim = 256\n",
        "outdim = 10\n",
        "x = tc.randn((no_examples,indim))\n",
        "model = BaseClassifier(indim,featuredim,outdim)\n",
        "output = model(x)\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "target = tc.tensor([0,3,2,8,2,9,3,7,1,6])\n",
        "computedloss = loss(output,target)\n",
        "computedloss.backward()\n",
        "\n",
        "for p in model.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "from torch import optim\n",
        "lr = 1e-3\n",
        "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XUMczflHXB",
        "outputId": "9786fb54-de7f-425d-a79b-2e3eef53bbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 784])\n",
            "torch.Size([256])\n",
            "torch.Size([10, 256])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    }
  ]
}